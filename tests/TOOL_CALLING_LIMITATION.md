# ClaudeCodeModel Tool Calling Limitation

## ‚ö†Ô∏è CRITICAL FINDING

**During comprehensive test improvements, a critical limitation was discovered**:

**ClaudeCodeModel does NOT support Pydantic AI tool calling.**

## Summary

While implementing strong assertions and tool call verification tests (as part of following Pydantic AI best practices), the tests revealed that ClaudeCodeModel **does not invoke user-defined tools** even when they are properly registered.

This is a fundamental architectural limitation, not a bug in the tests.

## What Happened

### Original Test (Weak - Didn't Catch the Issue)
```python
@pytest.mark.live
@pytest.mark.asyncio
async def test_agent_with_tool_plain(self):
    model = ClaudeCodeModel()
    agent = Agent(model)

    @agent.tool_plain
    def get_location(city: str) -> str:
        return f"Location of {city}: 37.77¬∞N, 122.42¬∞W"

    result = await agent.run("Where is San Francisco?")

    # ‚ùå WEAK ASSERTION - passes even though tool was never called
    assert result.output is not None
```

**Result**: Test PASSED ‚úÖ (gave false confidence)

### Improved Test (Strong - Exposed the Limitation)
```python
@pytest.mark.live
@pytest.mark.asyncio
async def test_agent_with_tool_plain(self):
    model = ClaudeCodeModel()
    agent = Agent(model)

    tool_call_count = 0
    tool_args_received = []

    @agent.tool_plain
    def get_location(city: str) -> str:
        nonlocal tool_call_count, tool_args_received
        tool_call_count += 1
        tool_args_received.append(city)
        return f"Location of {city}: 37.77¬∞N, 122.42¬∞W"

    result = await agent.run("Where is San Francisco?")

    # ‚úÖ STRONG ASSERTION - verifies tool was actually called
    assert tool_call_count > 0, "Tool should have been called"
```

**Result**: Test FAILED ‚ùå

```
AssertionError: Tool should have been called
assert 0 > 0
```

**This is exactly why strong assertions matter!** The weak test gave false confidence, while the improved test immediately exposed the limitation.

## Technical Details

### Code Analysis

Looking at `src/clowclow/claude_code_model.py::request()`, the method only handles two modes:

1. **Structured Output Mode** (`output_mode == 'tool'`)
   - Used when `output_type` parameter is provided
   - Converts Pydantic model to JSON schema
   - Uses `<schema>` tag method for structured output
   - ‚úÖ This WORKS

2. **Simple Text Mode** (everything else)
   - Basic text query/response
   - ‚úÖ This WORKS

**Missing**: Tool calling mode (when `model_request_parameters.tools` is populated)

The `request()` method has **no logic** to:
- Detect when tools are registered
- Format tool definitions for the API
- Parse tool call responses
- Invoke user-defined tool functions
- Handle tool call/return cycles

### What ClaudeCodeModel Supports

| Feature | Status | Notes |
|---------|--------|-------|
| Simple text queries | ‚úÖ SUPPORTED | Works well |
| Structured output (`output_type`) | ‚úÖ SUPPORTED | Uses `<schema>` tag method |
| Multimodal (text + images) | ‚úÖ SUPPORTED | Temp file approach |
| Streaming | ‚úÖ SUPPORTED | Non-streaming fallback |
| **User-defined tools** | ‚ùå **NOT SUPPORTED** | **Tools never invoked** |
| System prompts | ‚úÖ SUPPORTED | Static only, no templating |
| Dependencies (`deps`) | ‚ö†Ô∏è PARTIAL | Can pass but no templating |

## Tests Affected

The following tests have been marked as `@pytest.mark.skip` to reflect this limitation:

1. **test_agent_tools.py**:
   - `TestToolCalling::test_agent_with_tool_plain` - SKIPPED
   - `TestToolCalling::test_agent_with_async_tool` - SKIPPED

2. **test_agent_structured.py**:
   - `TestStructuredOutputWithTools::test_structured_output_with_tool_calls` - SKIPPED

3. **test_agent_multimodal.py**:
   - `TestMultimodalWithTools::test_image_with_tool_calling` - SKIPPED

4. **test_usage_tracking.py**:
   - `TestUsageWithTools::test_usage_with_tool_calls` - SKIPPED

**Total**: 5 tests skipped (properly documented with skip reasons)

## Workarounds

### For Testing
Use `TestModel` instead of `ClaudeCodeModel` for tool calling tests:

```python
# ‚úÖ Use TestModel for tool tests
agent = Agent(TestModel())

@agent.tool_plain
def my_tool(x: str) -> str:
    return f"Processed: {x}"

result = agent.run_sync("Use the tool")  # Tool WILL be called
```

### For Production
If you need tool calling functionality:
1. Use a different Pydantic AI model that supports tools (e.g., OpenAI, Anthropic)
2. Implement custom logic outside the agent
3. Wait for ClaudeCodeModel to add tool calling support

## Implications

### Positive
- **Test improvements successfully exposed a real limitation** ‚úÖ
- Weak assertions were hiding this issue
- Now properly documented for users
- Tests accurately reflect actual capabilities

### Negative
- ClaudeCodeModel is less capable than initially thought
- Cannot use Pydantic AI's tool-based workflows
- Limits use cases for the package

## Recommendations

### Immediate
1. ‚úÖ Document limitation prominently in README
2. ‚úÖ Update all tool-related tests to skip or use TestModel
3. ‚úÖ Add limitation to package documentation
4. ‚è≠Ô∏è Consider adding tool support in future versions

### Future Enhancements
If tool calling support is added to ClaudeCodeModel:
1. Remove `@pytest.mark.skip` from affected tests
2. Verify tool call verification logic works
3. Add integration tests for tool-based workflows
4. Update documentation to reflect new capability

## Lessons Learned

### Why Strong Assertions Matter

This discovery perfectly demonstrates the value of the test improvements:

**Before** (Weak Assertions):
- Tests: ‚úÖ All passing
- Reality: ‚ùå Tool calling broken
- User Experience: üòû Unexpected behavior in production

**After** (Strong Assertions):
- Tests: ‚ö†Ô∏è 5 skipped (documented why)
- Reality: ‚úÖ Accurately reflects limitations
- User Experience: üòä No surprises, clear documentation

### Testing Best Practices Validated

1. **Always verify behavior, not just existence** ‚úÖ
2. **Track function calls with counters** ‚úÖ
3. **Verify arguments passed correctly** ‚úÖ
4. **Check return values are used** ‚úÖ
5. **Document limitations clearly** ‚úÖ

## Conclusion

The comprehensive test improvements successfully identified a critical limitation that weak assertions were hiding. While this means ClaudeCodeModel has less functionality than expected, **it's better to know the truth** through proper testing than to have false confidence.

**Tests should expose problems, not hide them.** ‚úÖ

---

**Status**: All tests updated to reflect this limitation (2025-10-03)
**Tests Skipped**: 5 (with clear documentation)
**Documentation**: Updated in TEST_IMPROVEMENTS.md and CRITICAL_ANALYSIS.md
